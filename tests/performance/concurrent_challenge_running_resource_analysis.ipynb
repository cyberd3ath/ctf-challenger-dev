{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-23T17:34:26.359370Z",
     "start_time": "2025-09-23T17:30:41.208084Z"
    }
   },
   "source": [
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(os.path.join(Path().resolve(), \"utils\"))\n",
    "\n",
    "from TestUser import TestUser\n",
    "from get_resource_status import get_resource_status\n",
    "\n",
    "\n",
    "challenge_id = 1\n",
    "\n",
    "tries_per_amount = 1\n",
    "\n",
    "min_concurrent_instances = 1\n",
    "max_concurrent_instances = 5\n",
    "step_concurrent_instances = 1\n",
    "\n",
    "duration_seconds_per_test = 10\n",
    "resource_status_polling_interval_seconds = 1\n",
    "\n",
    "timeout_until_measurement_seconds = 10\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=[\n",
    "    'concurrent_instances',\n",
    "    'try_number',\n",
    "    'average_cpu_percent',\n",
    "    'max_cpu_percent',\n",
    "    'min_cpu_percent',\n",
    "    'average_memory_percent',\n",
    "    'max_memory_percent',\n",
    "    'min_memory_percent'\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "for amount in range(min_concurrent_instances, max_concurrent_instances + 1, step_concurrent_instances):\n",
    "    for try_number in range(1, tries_per_amount + 1):\n",
    "        print(f'Starting test with {amount} concurrent instances, try {try_number}...')\n",
    "\n",
    "        users = []\n",
    "\n",
    "        for i in range(amount):\n",
    "            user = TestUser(\n",
    "                username=f'testuser_{i+1}',\n",
    "                password='testpass',\n",
    "                email=f'test{i+1}@test.com'\n",
    "            )\n",
    "            users.append(user)\n",
    "\n",
    "        for user in users:\n",
    "            user.create()\n",
    "\n",
    "        for user in users:\n",
    "            user.launch_challenge(challenge_id)\n",
    "\n",
    "        print('Waiting for all challenges to be running...')\n",
    "        time.sleep(timeout_until_measurement_seconds)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        print('Collecting resource status data...')\n",
    "        cpu_percents = []\n",
    "        memory_percents = []\n",
    "        while True:\n",
    "            status = get_resource_status()\n",
    "            if status is None:\n",
    "                print('Failed to get resource status, retrying...')\n",
    "                time.sleep(resource_status_polling_interval_seconds)\n",
    "                continue\n",
    "\n",
    "            cpu_percents.append(status['cpu'] * 100)\n",
    "            memory_percent = status['memory']['used'] / status['memory']['total'] * 100\n",
    "            memory_percents.append(memory_percent)\n",
    "\n",
    "            while time.time() - start_time < resource_status_polling_interval_seconds:\n",
    "                pass\n",
    "\n",
    "            if time.time() - start_time >= duration_seconds_per_test:\n",
    "                break\n",
    "\n",
    "        for user in users:\n",
    "            user.stop_challenge(challenge_id)\n",
    "\n",
    "        for user in users:\n",
    "            user.delete()\n",
    "\n",
    "        average_cpu_percent = sum(cpu_percents) / len(cpu_percents)\n",
    "        max_cpu_percent = max(cpu_percents)\n",
    "        min_cpu_percent = min(cpu_percents)\n",
    "\n",
    "        average_memory_percent = sum(memory_percents) / len(memory_percents)\n",
    "        max_memory_percent = max(memory_percents)\n",
    "        min_memory_percent = min(memory_percents)\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame([{\n",
    "            'concurrent_instances': amount,\n",
    "            'try_number': try_number,\n",
    "            'average_cpu_percent': average_cpu_percent,\n",
    "            'max_cpu_percent': max_cpu_percent,\n",
    "            'min_cpu_percent': min_cpu_percent,\n",
    "            'average_memory_percent': average_memory_percent,\n",
    "            'max_memory_percent': max_memory_percent,\n",
    "            'min_memory_percent': min_memory_percent\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "        print(f'Test with {amount} concurrent instances, try {try_number} completed.')\n",
    "        print(f'Average CPU Percent: {average_cpu_percent:.2f}%')\n",
    "        print(f'Max CPU Percent: {max_cpu_percent:.2f}%')\n",
    "        print(f'Min CPU Percent: {min_cpu_percent:.2f}%')\n",
    "        print(f'Average Memory Percent: {average_memory_percent:.2f}%')\n",
    "        print(f'Max Memory Percent: {max_memory_percent:.2f}%')\n",
    "        print(f'Min Memory Percent: {min_memory_percent:.2f}%')\n",
    "        print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test with 1 concurrent instances, try 1...\n",
      "Waiting for all challenges to be running...\n",
      "Collecting resource status data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42105/4085229176.py:99: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with 1 concurrent instances, try 1 completed.\n",
      "Average CPU Percent: 6.71%\n",
      "Max CPU Percent: 9.01%\n",
      "Min CPU Percent: 0.00%\n",
      "Average Memory Percent: 1.86%\n",
      "Max Memory Percent: 1.91%\n",
      "Min Memory Percent: 1.83%\n",
      "\n",
      "Starting test with 2 concurrent instances, try 1...\n",
      "Waiting for all challenges to be running...\n",
      "Collecting resource status data...\n",
      "Test with 2 concurrent instances, try 1 completed.\n",
      "Average CPU Percent: 12.41%\n",
      "Max CPU Percent: 15.30%\n",
      "Min CPU Percent: 0.00%\n",
      "Average Memory Percent: 2.15%\n",
      "Max Memory Percent: 2.26%\n",
      "Min Memory Percent: 2.07%\n",
      "\n",
      "Starting test with 3 concurrent instances, try 1...\n",
      "Waiting for all challenges to be running...\n",
      "Collecting resource status data...\n",
      "Test with 3 concurrent instances, try 1 completed.\n",
      "Average CPU Percent: 16.92%\n",
      "Max CPU Percent: 22.23%\n",
      "Min CPU Percent: 0.00%\n",
      "Average Memory Percent: 2.48%\n",
      "Max Memory Percent: 2.65%\n",
      "Min Memory Percent: 2.35%\n",
      "\n",
      "Starting test with 4 concurrent instances, try 1...\n",
      "Waiting for all challenges to be running...\n",
      "Collecting resource status data...\n",
      "Test with 4 concurrent instances, try 1 completed.\n",
      "Average CPU Percent: 23.31%\n",
      "Max CPU Percent: 28.39%\n",
      "Min CPU Percent: 10.86%\n",
      "Average Memory Percent: 2.83%\n",
      "Max Memory Percent: 3.03%\n",
      "Min Memory Percent: 2.62%\n",
      "\n",
      "Starting test with 5 concurrent instances, try 1...\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed to launch challenge ID 1: {\"success\":false,\"message\":\"Failed to launch challenge: HTTP 500\",\"error_code\":500}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mException\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 58\u001B[39m\n\u001B[32m     55\u001B[39m     user.create()\n\u001B[32m     57\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m user \u001B[38;5;129;01min\u001B[39;00m users:\n\u001B[32m---> \u001B[39m\u001B[32m58\u001B[39m     \u001B[43muser\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlaunch_challenge\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchallenge_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     60\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mWaiting for all challenges to be running...\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     61\u001B[39m time.sleep(timeout_until_measurement_seconds)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/ctf-challenger/tests/performance/utils/TestUser.py:26\u001B[39m, in \u001B[36mTestUser.launch_challenge\u001B[39m\u001B[34m(self, challenge_id, prints)\u001B[39m\n\u001B[32m     25\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mlaunch_challenge\u001B[39m(\u001B[38;5;28mself\u001B[39m, challenge_id, prints=\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m     \u001B[43mlaunch_challenge\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msession\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchallenge_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprints\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprints\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/ctf-challenger/tests/performance/utils/launch_challenge.py:25\u001B[39m, in \u001B[36mlaunch_challenge\u001B[39m\u001B[34m(session, challenge_id, prints)\u001B[39m\n\u001B[32m     22\u001B[39m response = session.post(LAUNCH_CHALLENGE_URL, json=params)\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m response.status_code != \u001B[32m200\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m response.json().get(\u001B[33m\"\u001B[39m\u001B[33msuccess\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFailed to launch challenge ID \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mchallenge_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse.text\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m prints:\n\u001B[32m     27\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[33mChallenge ID \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mchallenge_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m launched successfully\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mException\u001B[39m: Failed to launch challenge ID 1: {\"success\":false,\"message\":\"Failed to launch challenge: HTTP 500\",\"error_code\":500}"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1c60b262e80341c4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
